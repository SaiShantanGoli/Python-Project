# -*- coding: utf-8 -*-
"""Untitled5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1JSfgq4sb5LEkQWN9A4SqU6-dvhTXBA3j
"""

#importing the necessary libraries required
import itertools
import numpy as np
import pandas as pd
import seaborn as sns
#from dataprep.eda import *
import matplotlib.pyplot as plt

from sklearn.metrics import accuracy_score
from sklearn.model_selection import GridSearchCV
from sklearn.model_selection import train_test_split


from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.metrics import confusion_matrix,precision_recall_curve,auc,roc_auc_score,accuracy_score,recall_score,classification_report 

from sklearn.preprocessing import LabelEncoder
from sklearn.preprocessing import StandardScaler
from keras.models import Sequential
from keras.layers import Dense
from keras.layers import Dropout

from flask import Flask, jsonify, request


import collections

import sys
import warnings

if not sys.warnoptions:
    warnings.simplefilter("ignore")

def clean(df):
  '''
  I/P   : Dataframe of the training file
  O/P   : Cleaned dataframe after all the necessary operations
  Steps : 1. First we wil replace all the null values 
            with the NONE value

          2. First we will understand the dataframe
            then we will delete the not necessary columns
          
          3. Then we will convert all the string encoded 
            values to the int using label encoder
  '''
  #fill all null values with none
  df.fillna('None')
  #taking the necessary columns only
  df = df[['TRAN_AMT','ACCT_PRE_TRAN_AVAIL_BAL','CUST_AGE','OPEN_ACCT_CT','WF_dvc_age','CARR_NAME','RGN_NAME','STATE_PRVNC_TXT','ALERT_TRGR_CD','DVC_TYPE_TXT','CUST_ZIP','CUST_STATE','FRAUD_NONFRAUD']]
  #decalring the label encoder to encode the string values
  le = LabelEncoder()
  #Categorical boolean mask
  categorical_feature_mask = df.dtypes==object
  #filter categorical columns using mask and turn it into a list
  categorical_cols = df.columns[categorical_feature_mask].tolist()
  #apply le on categorical feature columns
  df[categorical_cols] = df[categorical_cols].apply(lambda col: le.fit_transform(col))
  print(le.transform(["Fraud","Non-Fraud"]))
  #scale the vlaues to one range
  return df

def split(df):
  '''
  I/P   : Dataframe after all pre processing
  O/P   : X_train y_train X_test y_test values that will be used for training
  Steps : 1. using train test split we will split the data
  '''
  #dividing X & Y
  X = df.drop('FRAUD_NONFRAUD',axis=1)
  Y = df['FRAUD_NONFRAUD']
  #calling the train test method of scikit learn
  X_train, X_test, y_train, y_test = train_test_split(X, Y, train_size=0.8, random_state=42,stratify=Y)

  return X_train, X_test, y_train, y_test

def plot_confusion_matrix(cm, classes,normalize=False,title='Confusion matrix',cmap=plt.cm.Blues):
    """
    I/P   : y_test & y_pred values
    O/P   : confusion matrix explaining the classification
    Steps : This function prints and plots the confusion matrix.
            Normalization can be applied by setting `normalize=True`.
    """
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=0)
    plt.yticks(tick_marks, classes)

    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        #print("Normalized confusion matrix")
    else:
        1#print('Confusion matrix, without normalization')
    #print(cm)

    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, cm[i, j],
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

def model_training(df):
  '''
  I/P   : Dataframe after reading the training file
  O/P   : model training as the output
  Steps : 1. we will call necessary functions for the 
             data pre processing and then train and test the model
  '''
  df = clean(df)
  X_train, X_test, y_train, y_test = split(df)
  xgb = GradientBoostingClassifier(
    max_features='auto',
    min_samples_leaf=1,
    n_estimators=400,
    learning_rate=0.5,
    max_depth=5,
    random_state=1,
    )
  xgb.fit(X_train,y_train)
  y_pred = xgb.predict(X_test)
  score = accuracy_score(y_test,y_pred)
  print()
  print("Accuracy on test data is ",score,'\n')
  print(classification_report(y_test,y_pred))
  cnf_matrix = confusion_matrix(y_test,y_pred)
  np.set_printoptions(precision=2)
  # Plot non-normalized confusion matrix
  class_names = ["Fraud","Non-Fraud"]
  plt.figure()
  plot_confusion_matrix(cnf_matrix
                        , classes=class_names
                        , title='Confusion matrix')
  plt.show()

df1 = pd.read_excel("trainset.xlsx")
model_training(df1)

def clean_nn(df):
  '''
  I/P   : Dataframe after reading the training file
  O/P   : scaled values for model training
  Steps : 1. we will call necessary functions for the 
             data pre processing and scale the values
  '''
  df = clean(df)
  X_train, X_test, y_train, y_test = split(df)
  sc = StandardScaler()
  X_train= sc.fit_transform(X_train)
  X_test = sc.fit_transform(X_test)
  return X_train, X_test, y_train, y_test

def model_training_nn(X_train, X_test, y_train, y_test):
  '''
  I/P   : X_train, X_test, y_train, y_test
  O/P   : model training as the output
  Steps : 1. we will create a neural network and fit it
              on the data and check accuracy
  '''
  clas = Sequential()
  clas.add(Dense(units=16,kernel_initializer='uniform',activation='relu',input_dim=12))
  clas.add(Dropout(rate=0.1))
  clas.add(Dense(units=32,kernel_initializer='uniform',activation='relu',input_dim=12))
  clas.add(Dropout(rate=0.1))
  clas.add(Dense(units=64,kernel_initializer='uniform',activation='relu',input_dim=12))
  clas.add(Dropout(rate=0.1))
  clas.add(Dense(units=2,kernel_initializer='uniform',activation='softmax'))
  clas.compile(optimizer = 'adam', loss = 'sparse_categorical_crossentropy', metrics = ['accuracy'])

  # train neural netork
  clas.fit(X_train,y_train,batch_size=10,epochs=5,verbose=2)

  # evalute model
  scores = clas.evaluate(X_test,y_test,verbose=0)

  # print results
  print('Error',scores[0],'\nAccuracy',scores[1])
  y_pred = []
  predictions = clas.predict([X_test])
  for i in range(len(predictions)):
    #print("data is ",predictions[i])
    y_pred.append(np.argmax(predictions[i]))
  print(classification_report(y_test,y_pred))
  cnf_matrix = confusion_matrix(y_test,y_pred)
  np.set_printoptions(precision=2)
  # Plot non-normalized confusion matrix
  class_names = ["Fraud","Non-Fraud"]
  plt.figure()
  plot_confusion_matrix(cnf_matrix
                        , classes=class_names
                        , title='Confusion matrix')
  plt.show()

def NN(df):
  X_train, X_test, y_train, y_test = clean_nn(df)
  model_training_nn(X_train, X_test, y_train, y_test)
  #plot(clas,X_test)

NN(df1)

"""**'''# Randomn Under sampling'''**"""

def equal_sample(df):
  '''
  I/P   : data frame will be the input
  O/P   : data frame with both the classes
          of same count will be the output
  Steps : 1. we will take the minorty class and count the 
             number of values present in it and take the 
             same exact count of majority class so that 
             both will be in equal number
  '''
  fraud_df = df.loc[df['FRAUD_NONFRAUD'] == "Fraud"]
  #len(fraud_df)
  nonfraud_df = df.loc[df['FRAUD_NONFRAUD'] == "Non-Fraud"][:len(fraud_df)]

  equal_df = pd.concat([fraud_df, nonfraud_df])

  return equal_df

def model_training_u(df):
  '''
  I/P   : Dataframe after taking equal counts from 
          both the categories
  O/P   : model training as the output
  Steps : 1. we will call necessary functions for the 
             data pre processing and then train and test the model
  '''
  df = equal_sample(df)
  model_training(df)
  model_training_nn
  X_train1, X_test1, y_train1, y_test1 = split(df)

model_training_u(df1)


# initialize our Flask application
app= Flask(__name__)
df1 = pd.read_excel("trainset.xlsx")

#trainig endpoint
@app.route("/training", methods=["GET"])
def train():
    model_training(df1)
    return jsonify(" Training started !!!")

#testing endpoint
@app.route("/predict", methods=["GET"])
def predict():
    test_data = request.get_json()
    name = predict('test_data')
    return jsonify(" The output is !!! ",name)

